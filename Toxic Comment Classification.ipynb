{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training 6 models, 1 For each Label.\n",
    "Using an RNN with LSTM\n",
    "Framework: Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Input, Dropout, Dense, LSTM, Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words=['about','above','after','again','against','all','am','an','and','any','are','as','at','be','because','been','before'\n",
    "            ,'being','below','between','both','by','can','could','did','do','does','doing','don','down','during','each','few','for',\n",
    "            'from','further','had','has','have','having','he','ll','her','here','hers','herself','him','himself','his','how',\n",
    "            'how','i','ve','if','in','into','is','it','its','itself','let','me','more','most','must','my','myself','of','off',\n",
    "            'on','once','only','or','other','ought','our','ours','ourselves','out','over','own','same','shall','she','should',\n",
    "            'so','some','such','than','that','the','their','theirs','them','themselves','then','there','these','they','re',\n",
    "            'this','those','through','to','too','under','until','up','very','was','we','were','what','when','where','which',\n",
    "            'while','who','whom','why','with','would','you','your','yours','yourself',\n",
    "            'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "def remove_punctuation(snt):\n",
    "    return re.sub(r'[\\W]',' ',str(snt).lower())\n",
    "\n",
    "def remove_stop_words(sent):\n",
    "    words = sent.split()\n",
    "    resultwords=[word.lower() for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(resultwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data():\n",
    "    train.comment_text=train.comment_text.apply(remove_punctuation)\n",
    "    test.comment_text=test.comment_text.apply(remove_punctuation)\n",
    "    train.comment_text=train.comment_text.apply(remove_stop_words)\n",
    "    test.comment_text=test.comment_text.apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Transforming text to seq...\n"
     ]
    }
   ],
   "source": [
    "text_data=np.hstack([train.comment_text.str.lower(), \n",
    "                      test.comment_text.str.lower()])\n",
    "tok=Tokenizer()\n",
    "tok.fit_on_texts(text_data)\n",
    "print(\"   Transforming text to seq...\")\n",
    "train[\"input\"] = tok.texts_to_sequences(train.comment_text.str.lower())\n",
    "test[\"input\"] = tok.texts_to_sequences(test.comment_text.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 348630\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 75\n",
    "MAX_TOKEN = np.max([np.max(train.input.max()),np.max(test.input.max())]) + 180\n",
    "print(MAX_LENGTH, MAX_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperating Training and Cross Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.input.to_csv(\"train_data.csv\",index=False)\n",
    "test.input.to_csv(\"test_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data=train[:-1000]\n",
    "valid_data=train[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = pad_sequences(train_data.input, maxlen=MAX_LENGTH)\n",
    "valid_x = pad_sequences(valid_data.input, maxlen=MAX_LENGTH)\n",
    "test_x  = pad_sequences(test.input, maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df=pd.read_csv(\"submission.csv\")\n",
    "sub_df.id=test.id\n",
    "sub_df.to_csv(\"Submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y1 = np_utils.to_categorical(train_data.toxic.values, 2)\n",
    "valid_y1 = np_utils.to_categorical(valid_data.toxic.values, 2)\n",
    "\n",
    "train_y2 = np_utils.to_categorical(train_data.severe_toxic.values, 2)\n",
    "valid_y2 = np_utils.to_categorical(valid_data.severe_toxic.values, 2)\n",
    "\n",
    "train_y3 = np_utils.to_categorical(train_data.obscene.values, 2)\n",
    "valid_y3 = np_utils.to_categorical(valid_data.obscene.values, 2)\n",
    "\n",
    "train_y4 = np_utils.to_categorical(train_data.threat.values, 2)\n",
    "valid_y4 = np_utils.to_categorical(valid_data.threat.values, 2)\n",
    "\n",
    "train_y5 = np_utils.to_categorical(train_data.insult.values, 2)\n",
    "valid_y5 = np_utils.to_categorical(valid_data.insult.values, 2)\n",
    "\n",
    "train_y6 = np_utils.to_categorical(train_data.identity_hate.values, 2)\n",
    "valid_y6 = np_utils.to_categorical(valid_data.identity_hate.values, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM- RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = Input(shape=[MAX_LENGTH], name=\"in\")\n",
    "B = Embedding(MAX_TOKEN, 128)(A)\n",
    "C = LSTM(90) (B)\n",
    "D = Dropout(0.6) (Dense(128, activation='relu') (C))\n",
    "E = Dropout(0.4) (Dense(32, activation='relu') (D))\n",
    "F = Dropout(0.4) (Dense(8, activation='relu') (E))\n",
    "output = Dense(2, activation=\"softmax\") (F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = Model(A, output)\n",
    "model2 = Model(A, output)\n",
    "model3 = Model(A, output)\n",
    "model4 = Model(A, output)\n",
    "model5 = Model(A, output)\n",
    "model6 = Model(A, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 75, 128)           44624640  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 90)                78840     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               11648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 44,719,538\n",
      "Trainable params: 44,719,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model1.summary())\n",
    "# print(model2.summary())\n",
    "# print(model3.summary())\n",
    "# print(model4.summary())\n",
    "# print(model5.summary())\n",
    "# print(model6.summary())\n",
    "ad=Adam(0.001,decay=1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1.compile(optimizer=ad,loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "model2.compile(optimizer=ad,loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "model3.compile(optimizer=ad,loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "model4.compile(optimizer=ad,loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "model5.compile(optimizer=ad,loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "model6.compile(optimizer=ad,loss='categorical_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 158571 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "158571/158571 [==============================] - 1886s - loss: 0.1967 - acc: 0.9453 - val_loss: 0.0851 - val_acc: 0.9690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e093275fd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(train_x, train_y1, batch_size = 128, epochs = 1, \n",
    "                verbose = 1, validation_data = (valid_x, valid_y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " _model1=model1.to_json()\n",
    "with open(\"Model1.json\",\"w\") as json_file:\n",
    "    json_file.write(_model1)\n",
    "model1.save_weights(\"weights1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_file = open(\"Model1.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"weights1.h5\")\n",
    "model1.compile(optimizer=ad,loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "preds=model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(preds,columns=['no','yes'])\n",
    "sub_df.toxic=df.yes\n",
    "sub_df.to_csv(\"Submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 158571 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "158571/158571 [==============================] - 1430s - loss: 0.0363 - acc: 0.9897 - val_loss: 0.0215 - val_acc: 0.9920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0a00fd780>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_x, train_y2, batch_size = 128, epochs = 1, \n",
    "                verbose = 1, validation_data = (valid_x, valid_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " _model2=model2.to_json()\n",
    "with open(\"Model2.json\",\"w\") as json_file:\n",
    "    json_file.write(_model2)\n",
    "model2.save_weights(\"weights2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_file = open(\"Model2.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"weights2.h5\")\n",
    "model1.compile(optimizer=ad,loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "preds=model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(preds,columns=['no','yes'])\n",
    "sub_df.severe_toxic=df.yes\n",
    "sub_df.to_csv(\"Submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 158571 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "158571/158571 [==============================] - 1431s - loss: 0.0723 - acc: 0.9754 - val_loss: 0.0367 - val_acc: 0.9860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0a80f26a0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(train_x, train_y3, batch_size = 128, epochs = 1, \n",
    "                verbose = 1, validation_data = (valid_x, valid_y3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_model3=model3.to_json()\n",
    "with open(\"Model3.json\",\"w\") as json_file:\n",
    "    json_file.write(_model3)\n",
    "model3.save_weights(\"weights3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_file = open(\"Model3.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"weights3.h5\")\n",
    "model1.compile(optimizer=ad,loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "preds=model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(preds,columns=['no','yes'])\n",
    "sub_df.obscene=df.yes\n",
    "sub_df.to_csv(\"Submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 158571 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "158571/158571 [==============================] - 1429s - loss: 0.0170 - acc: 0.9969 - val_loss: 0.0118 - val_acc: 0.9960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0a9770b38>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(train_x, train_y4, batch_size = 128, epochs = 1, \n",
    "                verbose = 1, validation_data = (valid_x, valid_y4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " _model4=model4.to_json()\n",
    "with open(\"Model4.json\",\"w\") as json_file:\n",
    "    json_file.write(_model4)\n",
    "model4.save_weights(\"weights4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_file = open(\"Model4.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"weights4.h5\")\n",
    "model1.compile(optimizer=ad,loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "preds=model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(preds,columns=['no','yes'])\n",
    "sub_df.threat=df.yes\n",
    "sub_df.to_csv(\"Submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 158571 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "158571/158571 [==============================] - 2002s - loss: 0.0932 - acc: 0.9648 - val_loss: 0.0540 - val_acc: 0.9770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0d090b710>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(train_x, train_y5, batch_size = 128, epochs = 1, \n",
    "                verbose = 1, validation_data = (valid_x, valid_y5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_model5=model5.to_json()\n",
    "with open(\"Model5.json\",\"w\") as json_file:\n",
    "    json_file.write(_model5)\n",
    "model5.save_weights(\"weights5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_file = open(\"Model5.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"weights5.h5\")\n",
    "model1.compile(optimizer=ad,loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "preds=model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(preds,columns=['no','yes'])\n",
    "sub_df.insult=df.yes\n",
    "sub_df.to_csv(\"Submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 158571 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "158571/158571 [==============================] - 55136s - loss: 0.0327 - acc: 0.9909 - val_loss: 0.0312 - val_acc: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0d3b9fc88>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.fit(train_x, train_y6, batch_size = 128, epochs = 1, \n",
    "                verbose = 1, validation_data = (valid_x, valid_y6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " _model6=model6.to_json()\n",
    "with open(\"Model6.json\",\"w\") as json_file:\n",
    "    json_file.write(_model6)\n",
    "model6.save_weights(\"weights6.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_file = open(\"Model6.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"weights6.h5\")\n",
    "model1.compile(optimizer=ad,loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "preds=model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(preds,columns=['no','yes'])\n",
    "sub_df.identity_hate=df.yes\n",
    "sub_df.to_csv(\"Submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
